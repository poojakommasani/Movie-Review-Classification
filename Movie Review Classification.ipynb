{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "['neg', 'pos']\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "# Total reviews\n",
    "print (len(movie_reviews.fileids())) # Output: 2000\n",
    " \n",
    "# Review categories\n",
    "print (movie_reviews.categories()) # Output: [u'neg', u'pos']\n",
    " \n",
    "# Total positive reviews\n",
    "print (len(movie_reviews.fileids('pos'))) # Output: 1000\n",
    " \n",
    "# Total negative reviews\n",
    "print (len(movie_reviews.fileids('neg'))) # Output: 1000\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    " \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "# clean words, i.e. remove stopwords and punctuation\n",
    "def clean_words(words, stopwords_english):\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            words_clean.append(word)    \n",
    "    return words_clean \n",
    " \n",
    "# feature extractor function for unigram\n",
    "def bag_of_words(words):    \n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "# feature extractor function for ngrams (bigram)\n",
    "def bag_of_ngrams(words, n=2):\n",
    "    words_ng = []\n",
    "    for item in iter(ngrams(words, n)):\n",
    "        words_ng.append(item)\n",
    "    words_dictionary = dict([word, True] for word in words_ng)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination of Unigrams and Bigrams Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': True, 'movie': True, ('very', 'good'): True, ('good', 'movie'): True}\n"
     ]
    }
   ],
   "source": [
    " # let's define a new function that extracts all features\n",
    "# i.e. that extracts both unigram and bigrams features\n",
    "def bag_of_all_words(words, n=2):\n",
    "    words_clean = clean_words(words, stopwords_english)\n",
    "    words_clean_for_bigrams = clean_words(words, stopwords_english_for_bigrams)\n",
    " \n",
    "    unigram_features = bag_of_words(words_clean)\n",
    "    bigram_features = bag_of_ngrams(words_clean_for_bigrams)\n",
    " \n",
    "    all_features = unigram_features.copy()\n",
    "    all_features.update(bigram_features)\n",
    " \n",
    "    return all_features\n",
    " \n",
    "print (bag_of_all_words(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    " \n",
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append(words)\n",
    "    \n",
    "neg_reviews = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    neg_reviews.append(words)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive reviews feature set\n",
    "pos_reviews_set = []\n",
    "for words in pos_reviews:\n",
    "    pos_reviews_set.append((bag_of_all_words(words), 'pos'))\n",
    "    \n",
    "    \n",
    "# negative reviews feature set\n",
    "neg_reviews_set = []\n",
    "for words in neg_reviews:\n",
    "    neg_reviews_set.append((bag_of_all_words(words), 'neg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n",
      "200 1800\n"
     ]
    }
   ],
   "source": [
    "print (len(pos_reviews_set), len(neg_reviews_set)) # Output: (1000, 1000)\n",
    " \n",
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "from random import shuffle \n",
    "shuffle(pos_reviews_set)\n",
    "shuffle(neg_reviews_set)\n",
    " \n",
    "test_set = pos_reviews_set[:100] + neg_reviews_set[:100]\n",
    "train_set = pos_reviews_set[100:] + neg_reviews_set[100:]\n",
    " \n",
    "print(len(test_set),  len(train_set)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) \n",
    " \n",
    "#print (classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix [[67 33]\n",
      " [ 5 95]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "testing_set_content=[i[0] for i in test_set]\n",
    "golden_label=[i[1] for i in test_set]\n",
    "tested_label=classifier.classify_many(testing_set_content)\n",
    "#print (golden_label)\n",
    "cm = confusion_matrix(golden_label,tested_label) \n",
    "print (\"Confusion matrix\",cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.67      0.78       100\n",
      "         pos       0.74      0.95      0.83       100\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.84      0.81      0.81       200\n",
      "weighted avg       0.84      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(golden_label, tested_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "<ProbDist with 2 samples>\n",
      "neg\n",
      "0.9236562814579945\n",
      "0.07634371854200557\n",
      "pos\n",
      "<ProbDist with 2 samples>\n",
      "pos\n",
      "0.009277879420519029\n",
      "0.9907221205794827\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "custom_review = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_review_tokens = word_tokenize(custom_review)\n",
    "custom_review_set = bag_of_all_words(custom_review_tokens)\n",
    "print (classifier.classify(custom_review_set)) # Output: neg\n",
    "# Negative review correctly classified as negative\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_review_set)\n",
    "print (prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (prob_result.max()) # Output: neg\n",
    "print (prob_result.prob(\"neg\")) # Output: 0.770612685688\n",
    "print (prob_result.prob(\"pos\")) # Output: 0.229387314312\n",
    "\n",
    "\n",
    "custom_review = \"It was a wonderful and amazing movie. I loved it. Best direction, good acting.\"\n",
    "custom_review_tokens = word_tokenize(custom_review)\n",
    "custom_review_set = bag_of_all_words(custom_review_tokens)\n",
    " \n",
    "print (classifier.classify(custom_review_set)) # Output: pos\n",
    "# Positive review correctly classified as positive\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_review_set)\n",
    "print (prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (prob_result.max()) # Output: pos\n",
    "print (prob_result.prob(\"neg\")) # Output: 0.00677736186354\n",
    "print (prob_result.prob(\"pos\")) # Output: 0.993222638136\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
